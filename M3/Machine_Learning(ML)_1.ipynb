{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9928d5c-a285-4b50-9c34-064b8f4278a5",
   "metadata": {},
   "source": [
    "## 머신러닝(Machine Learning)과 딥러닝(Deep Learning)\n",
    "\n",
    "### 1. 머신러닝 (Machine Learning)\n",
    "- **구조**: 입력(X) → (★)특징추출(Feature Extraction) → 분류기(Classifier) → 출력(Y)\n",
    "- 특징 추출을 위한 알고리즘을 **사람이 직접 설계**해야 함  \n",
    "- 컴퓨터는 사람이 정의한 특징을 이용해 학습하는 방식  \n",
    "- 특정 도메인에 대한 지식과 알고리즘 구축을 위한 노력이 필요함\n",
    "\n",
    "**예시**\n",
    "- 이미지 분류 시, 사람이 직접 윤곽선·색상·형태 등의 특징을 정의하여 모델에 제공\n",
    "- SVM, KNN, 결정트리 등의 전통적인 머신러닝 모델에서 주로 사용\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 딥러닝 (Deep Learning)\n",
    "- **구조**: 입력(X) → 블랙박스(신경망) → 출력(Y)\n",
    "- 특징 추출 과정까지 **모델이 자동으로 학습**\n",
    "- 사람의 개입 없이 데이터로부터 유의미한 특징 표현을 스스로 학습함\n",
    "\n",
    "**예시**\n",
    "- CNN이 이미지의 엣지 → 패턴 → 객체 구조를 자동으로 학습\n",
    "- RNN이 시계열 데이터나 문장 구조를 자동으로 파악\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a50458-7fc0-412b-91be-8cbf97b3ff8d",
   "metadata": {},
   "source": [
    "## 머신러닝 학습의 기본 개념 정리\n",
    "\n",
    "### 1. 입력과 출력의 관계\n",
    "- **입력 데이터**:  \n",
    "  $X = (x_1, x_2, \\dots, x_n)$\n",
    "\n",
    "- **목표 출력(정답)**:  \n",
    "  $Y = (y_1, y_2, \\dots, y_n)$\n",
    "\n",
    "모델은 입력 $X$를 받아 예측값 $Y'$를 생성하고,  \n",
    "이 예측값이 실제 정답 $Y$와 얼마나 다른지를 계산하여 **오차(error)**를 구한다.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 모델 구조와 파라미터\n",
    "- 모델 내부에는 여러 **파라미터(parameter)** 존재  \n",
    "  예: $w_0, w_1, \\dots, w_n$\n",
    "- 처음에는 무작위(random) 값으로 설정  \n",
    "- 학습을 통해 오차를 줄이는 방향으로 파라미터를 조정한다.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 학습(Training) 과정의 흐름\n",
    "\n",
    "1. 입력 데이터 $X$를 모델에 전달  \n",
    "2. 모델이 현재 파라미터를 이용해 예측값 $Y'$ 계산  \n",
    "3. **손실함수(Loss Function)** 를 통해 오차 계산  \n",
    "   $$\n",
    "   L = (Y' - Y)^2\n",
    "   $$\n",
    "4. 오차가 최소가 되도록 파라미터를 **최적화(Optimization)**  \n",
    "   → 경사하강법(Gradient Descent) 등의 알고리즘 사용  \n",
    "5. 위 과정을 반복하여 최적의 파라미터를 찾는다.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 데이터 구성 예시\n",
    "\n",
    "| 입력 데이터 (X) | 정답 (Y) |\n",
    "|:--:|:--:|\n",
    "| $x_1$ | $y_1$ |\n",
    "| $x_2$ | $y_2$ |\n",
    "| ... | ... |\n",
    "| $x_n$ | $y_n$ |\n",
    "\n",
    "입력과 정답 쌍을 반복적으로 학습하며,  \n",
    "모델은 점점 더 정확한 예측값을 출력하게 된다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c8985-2d5c-43ff-ace3-a73b47e7a71e",
   "metadata": {},
   "source": [
    "## 훈련(Training), 검증(Validation), 테스트(Test) 데이터\n",
    "\n",
    "### 1. 훈련(Training) 데이터\n",
    "- 모델의 **파라미터(parameter)** 를 학습시키는 데 사용되는 데이터\n",
    "- 즉, 모델이 입력과 출력의 관계를 배우는 데 직접 참여함\n",
    "- 전체 데이터 중 대부분을 차지\n",
    "\n",
    "> 예: 모델이 가중치 \\( w_1, w_2, \\dots, w_n \\) 을 조정하며 패턴을 학습\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 검증(Validation) 데이터\n",
    "- 학습 과정 중 **과적합(overfitting)** 여부를 점검하고,  \n",
    "  **최적의 하이퍼파라미터(hyperparameter)** 를 찾기 위해 사용\n",
    "- 훈련 데이터의 일부를 분리하여 사용\n",
    "- 모델의 일반화 성능을 미리 확인할 수 있음\n",
    "\n",
    "> 예: 학습률(learning rate), 은닉층 수, 정규화 계수 등을 조정할 때 검증 데이터의 성능을 기준으로 선택\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 테스트(Test) 데이터\n",
    "- 학습이 완료된 후, 모델의 **최종 성능을 평가**하기 위해 사용\n",
    "- 학습 및 검증 단계에서 전혀 사용되지 않은 데이터\n",
    "- 모델의 **일반화 능력**을 객관적으로 판단하는 지표 제공\n",
    "\n",
    "> 예: 실제 서비스 환경에서 모델이 새로운 데이터를 얼마나 잘 예측하는지 측정\n",
    "\n",
    "---\n",
    "\n",
    "### 데이터 분할 요약\n",
    "\n",
    "| 구분 | 사용 목적 | 데이터 사용 시점 | 포함 여부 | 주요 역할 |\n",
    "|:--:|:--|:--|:--:|:--|\n",
    "| **Training** | 모델 파라미터 학습 | 학습 단계 | 포함 | 학습에 직접 사용 |\n",
    "| **Validation** | 모델 튜닝 및 과적합 점검 | 학습 중간 | 일부 포함 | 하이퍼파라미터 최적화 |\n",
    "| **Test** | 최종 성능 평가 | 학습 이후 | 미포함 | 일반화 성능 평가 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600590ca-e664-42a4-999b-83c07369d4a6",
   "metadata": {},
   "source": [
    "## 모델 구축 과정\n",
    "\n",
    "### 1. 머신러닝 모델 선택\n",
    "- 해결하고자 하는 **문제 유형**에 맞는 최적의 모델을 선택  \n",
    "  (예: 분류, 회귀, 군집 등)\n",
    "- 고려 요소  \n",
    "  - 사용 가능한 **훈련 데이터의 특성**  \n",
    "  - **목표 함수(objective function)** 및 문제의 목적\n",
    "- 대표적인 모델 예시  \n",
    "  - 선형모델, 결정트리, 신경망, SVM, 랜덤포레스트 등\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 모델 학습 (Training)\n",
    "- **훈련 데이터(Training data)** 를 사용하여 모델을 학습  \n",
    "- 파라미터를 조정해 입력과 출력의 관계를 학습함  \n",
    "- 대표적인 함수  \n",
    "  ```python\n",
    "  model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e602bfa-64b5-46ae-9d33-3b83b20ecf55",
   "metadata": {},
   "source": [
    "## 학습률 (Learning Rate)\n",
    "\n",
    "### 1. 개념\n",
    "- **학습률(η, learning rate)** 은 **모델이 파라미터를 얼마나 빠르게 업데이트할지**를 결정하는 하이퍼파라미터이다.\n",
    "- 경사하강법(Gradient Descent)에서 학습률은 다음과 같이 사용된다:\n",
    "\n",
    "  $$\n",
    "  w := w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "  $$\n",
    "\n",
    "  여기서  \n",
    "  - \\( w \\): 파라미터(parameter)  \n",
    "  - \\( L \\): 손실함수(loss function)  \n",
    "  - \\( \\eta \\): 학습률(learning rate)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 학습률이 너무 작은 경우\n",
    "- 학습 속도가 느림 (수렴까지 시간이 오래 걸림)\n",
    "- 하지만 **최저점(최적값)에 도달했을 때는 안정적**으로 수렴함\n",
    "- 작은 학습률 → 안정적이지만 학습 효율이 낮음\n",
    "\n",
    "> 예: 완만하게 손실이 줄어드는 형태 (느리지만 안정적 수렴)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 학습률이 너무 큰 경우\n",
    "- 학습 속도는 빠르지만, 최적점 근처에서 **진동하거나 발산**할 위험이 있음  \n",
    "- 오히려 **수렴하지 못하고 오차가 남거나 커질 수 있음**\n",
    "\n",
    "> 예: 학습 초기에 손실이 급격히 줄다가, 이후에 불안정하게 진동\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 학습 스케줄 (Learning Schedule)\n",
    "- 학습 과정 중 **학습률을 점진적으로 조정하는 방법**\n",
    "- 일반적인 전략:\n",
    "  - **초기:** 학습률을 크게 설정하여 빠르게 학습 시작  \n",
    "  - **후반:** 오차가 줄어들수록 학습률을 작게 조정하여 안정된 수렴 유도\n",
    "- 이를 통해 **steady state(안정 상태)** 에서 잔여 오차를 최소화할 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 요약\n",
    "\n",
    "| 구분 | 특징 | 장점 | 단점 |\n",
    "|:--:|:--|:--|:--|\n",
    "| **학습률이 작을 때** | 수렴 속도 느림 | 안정적 수렴 | 학습 시간 증가 |\n",
    "| **학습률이 클 때** | 빠른 수렴 시도 | 초기 학습 빠름 | 발산·진동 위험 |\n",
    "| **적절한 학습률** | 균형 잡힌 수렴 | 효율적 학습 | 설정 난이도 있음 |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb9e57-a243-4064-a897-9dfcb66e5352",
   "metadata": {},
   "source": [
    "## k-최근접 이웃 (k-Nearest Neighbor, kNN) 알고리즘\n",
    "\n",
    "### 1. 개요\n",
    "- **kNN 알고리즘**은 주어진 샘플의 특성과 **가장 가까운 이웃(neighbor)** 들을 찾아  \n",
    "  그들의 **레이블(label)** 혹은 **평균값**을 기준으로 새로운 데이터를 분류 또는 예측하는 알고리즘이다.\n",
    "\n",
    "- 새로운 데이터 \\( x \\)가 주어졌을 때:\n",
    "  1. 학습 데이터 중 \\( x \\)와의 거리를 계산한다.  \n",
    "  2. 가장 가까운 **k개의 이웃**을 선택한다.  \n",
    "  3. 선택된 이웃들의 **레이블의 다수결**(분류) 또는 **평균값**(회귀)을 결과로 예측한다.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 주요 특징\n",
    "- 모델이 명시적으로 학습되지 않으며(**lazy learning**),  \n",
    "  **전체 학습 데이터를 저장한 뒤 예측 시점에 계산**한다.\n",
    "- 계산량이 크지만, **직관적이고 구현이 쉬운** 알고리즘이다.\n",
    "- 거리 측정 방식으로는 일반적으로 **유클리드 거리(Euclidean distance)** 를 사용한다.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 활용 예시\n",
    "- **추천 시스템(Recommendation System)** 에서 자주 사용됨\n",
    "  - 사용자의 취향(특성)을 벡터로 표현하여, **가장 유사한 사용자** 또는 **아이템**을 탐색  \n",
    "  - 유사한 성향을 가진 사용자들이 선호하는 품목을 추천\n",
    "\n",
    "> 예시:  \n",
    "> 사용자의 독서 이력을 기반으로, 비슷한 취향을 가진 사람들의 평가가 높은 책을 추천\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 협업 필터링 (Collaborative Filtering)\n",
    "- kNN 개념은 협업 필터링의 기초로도 활용된다.\n",
    "\n",
    "#### (1) 사용자 기반 협업 필터링 (User-based)\n",
    "- 나와 **비슷한 사용자 그룹**을 찾고,  \n",
    "  그들이 선호하는 아이템 중 내가 아직 보지 않은 것을 추천\n",
    "\n",
    "#### (2) 아이템 기반 협업 필터링 (Item-based)\n",
    "- 특정 상품을 좋아하는 사람들이 **같이 자주 선택한 다른 상품**을 찾아 추천  \n",
    "- 아이템 간의 유사도(similarity)를 기준으로 그룹화하여 추천\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 요약\n",
    "\n",
    "| 구분 | 내용 |\n",
    "|:--|:--|\n",
    "| **학습 방식** | 비모수적(lazy learning), 별도 훈련 과정 없음 |\n",
    "| **핵심 개념** | 거리 기반의 이웃 탐색 |\n",
    "| **주요 하이퍼파라미터** | 이웃 수 \\( k \\), 거리 계산 방식 |\n",
    "| **대표 활용 분야** | 분류(Classification), 추천 시스템(Recommendation) |\n",
    "| **장점** | 단순하고 직관적, 소규모 데이터에 적합 |\n",
    "| **단점** | 계산 비용이 큼, 고차원 데이터에 비효율적 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e733cbf5-bc0f-41a1-969c-22c27920bd52",
   "metadata": {},
   "source": [
    "## k 값에 따른 영향 (Effect of k in k-Nearest Neighbor)\n",
    "\n",
    "### 1. k 값을 너무 작게 설정한 경우\n",
    "- 예: \\( k = 1 \\)\n",
    "- **특징**\n",
    "  - 예측 시 가장 가까운 한 개의 이웃만 고려\n",
    "  - 훈련 데이터의 **노이즈(Noise)** 에 매우 민감하게 반응\n",
    "  - 특정 샘플에 과도하게 적합될 가능성(과대적합, Overfitting)\n",
    "- **결과**\n",
    "  - 훈련 데이터에 대한 정확도는 높을 수 있으나,  \n",
    "    새로운 데이터에 대한 일반화 성능이 떨어짐\n",
    "\n",
    "> 요약: 과도한 예민함 → 과대적합 위험 증가\n",
    "\n",
    "---\n",
    "\n",
    "### 2. k 값을 너무 크게 설정한 경우\n",
    "- 많은 이웃의 평균값으로 예측을 수행\n",
    "- **특징**\n",
    "  - 노이즈(Noise)에는 강하지만,\n",
    "  - 너무 많은 데이터를 평균내므로 **정밀한 경계(boundary)** 를 구분하기 어려움\n",
    "- **결과**\n",
    "  - 모델이 지나치게 단순화되어 **과소적합(Underfitting)** 이 발생할 수 있음\n",
    "\n",
    "> 요약: 안정적이지만 단순화 → 과소적합 위험 증가\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 극단적인 경우 \\( k = N \\) (전체 샘플 수)\n",
    "- 모든 데이터를 평균하여 예측하므로, 항상 **전체 데이터의 평균값**을 출력\n",
    "- 분류 문제에서는 **가장 빈도수가 높은 클래스**로만 예측\n",
    "- 예를 들어, 영화 추천의 경우 평균적으로 가장 많이 본 영화(베스트셀러)만 추천하는 결과와 유사\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 정리\n",
    "\n",
    "| 구분 | 장점 | 단점 | 주요 문제 |\n",
    "|:--:|:--|:--|:--|\n",
    "| \\( k \\) 작을 때 | 높은 훈련 정확도 | 노이즈에 민감 | 과대적합 |\n",
    "| \\( k \\) 클 때 | 노이즈에 강함 | 경계가 모호, 일반화 과도 | 과소적합 |\n",
    "| \\( k = N \\) | 가장 안정적 | 항상 평균값만 예측 | 정보 손실 |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 용어 정리\n",
    "- **Noise(노이즈)**  \n",
    "  좋은 모델 학습에 도움이 되지 않는 **혼란스러운 정보**를 제공하는 샘플.  \n",
    "  즉, 데이터의 패턴을 왜곡시키는 이상치(outlier) 또는 불필요한 정보.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96920c70-02c5-4941-89cb-e7ad9e2ef0cb",
   "metadata": {},
   "source": [
    "## k-최근접 이웃 (k-Nearest Neighbor, kNN)의 장단점\n",
    "\n",
    "### 1. 장점\n",
    "- **훈련 시간이 거의 없음**\n",
    "  - 모델을 미리 학습시키지 않고, 예측 시점에 계산을 수행하는 방식  \n",
    "    → 별도의 학습 단계가 필요 없음\n",
    "- **알고리즘 개념이 단순함**\n",
    "  - 거리 계산만으로 동작하므로 이해와 구현이 쉬움\n",
    "- **하이퍼파라미터가 적음**\n",
    "  - 조정할 값이 거의 없으며, 핵심은 \\( k \\) 값 하나뿐\n",
    "- **특성(Feature)을 잘 선택하면 예측 성능이 우수**\n",
    "  - 적절한 특성 추출과 거리 척도 설정 시, 단순하지만 높은 정확도를 달성 가능\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 단점\n",
    "- **예측 시 계산 비용이 큼**\n",
    "  - 모든 학습 샘플과의 거리를 계산해야 하므로, 데이터가 많을수록 예측 속도가 느림\n",
    "- **새로운 데이터가 추가될 때마다 계산 재수행**\n",
    "  - 데이터셋이 변하면 거리 계산 결과도 달라짐 → 실시간 처리에 비효율적\n",
    "- **고차원 데이터에 비효율적**\n",
    "  - 차원이 증가할수록 거리 계산의 의미가 희석됨(차원의 저주, Curse of Dimensionality)\n",
    "- **Lazy Learning 특성**\n",
    "  - 학습 단계에서는 거의 계산하지 않지만, 예측 시점에 계산량이 많음  \n",
    "    → 실시간 서비스보다는 **소규모 데이터 분석**에 적합\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 요약\n",
    "\n",
    "| 구분 | 내용 |\n",
    "|:--:|:--|\n",
    "| **장점** | 학습단계 없음, 구현 간단, 파라미터 적음, 직관적 구조 |\n",
    "| **단점** | 예측 시 계산량 많음, 새로운 데이터 반영 어려움, 고차원 데이터에 비효율 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0c66a-e005-4975-8f4e-ef629385c0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
